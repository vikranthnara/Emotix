{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2 Demo: Complete MWB Tracking Pipeline\n",
        "\n",
        "This notebook demonstrates the complete Phase 2 pipeline:\n",
        "1. **Ingestion**: Load raw conversational text\n",
        "2. **Preprocessing**: Normalize slang, emojis, and apply fixes\n",
        "3. **Contextualization**: Create multi-turn sequences with context\n",
        "4. **Modeling**: Emotion classification and intensity prediction\n",
        "5. **Persistence**: Store results in SQLite\n",
        "\n",
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-06 12:57:38,727 - root - INFO - Starting Phase 2 Demo\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "from src.pipeline import run_full_pipeline\n",
        "from src.modeling import EmotionModel\n",
        "from src.contextualize import create_sequences_batch\n",
        "from src.persistence import MWBPersistence\n",
        "from src.validation import AmbiguityGenerator, MetricsTracker\n",
        "from src.utils import setup_logging\n",
        "\n",
        "# Setup logging\n",
        "setup_logging(log_level=\"INFO\")\n",
        "logging.info(\"Starting Phase 2 Demo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1-5: Complete Pipeline\n",
        "\n",
        "Run the full pipeline from ingestion to persistence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-06 12:57:38,732 - src.pipeline - INFO - Starting Phase 2 full pipeline\n",
            "2025-12-06 12:57:38,732 - src.pipeline - INFO - [1/5] Ingestion...\n",
            "2025-12-06 12:57:38,732 - src.ingest - INFO - Ingesting CSV from /Users/vikranthnara/Documents/GitHub/Emotix/data/sample_data.csv\n",
            "2025-12-06 12:57:38,736 - src.ingest - INFO - Validated DataFrame with 20 rows\n",
            "2025-12-06 12:57:38,770 - src.utils - INFO - Checkpoint saved: /Users/vikranthnara/Documents/GitHub/Emotix/checkpoints/ingestion_20251206_125738.parquet (20 rows)\n",
            "2025-12-06 12:57:38,770 - src.pipeline - INFO - [2/5] Preprocessing...\n",
            "2025-12-06 12:57:38,770 - src.preprocess - INFO - Loaded 33 slang/abbrev mappings\n",
            "2025-12-06 12:57:38,772 - src.preprocess - INFO - Preprocessing 20 rows\n",
            "2025-12-06 12:57:38,778 - src.preprocess - INFO - Preprocessing complete\n",
            "2025-12-06 12:57:38,781 - src.utils - INFO - Checkpoint saved: /Users/vikranthnara/Documents/GitHub/Emotix/checkpoints/preprocessing_20251206_125738.parquet (20 rows)\n",
            "2025-12-06 12:57:38,781 - src.pipeline - INFO - [3/5] Contextualization...\n",
            "2025-12-06 12:57:38,781 - src.persistence - INFO - Database schema initialized\n",
            "2025-12-06 12:57:38,782 - src.contextualize - INFO - Creating sequences for 20 utterances\n",
            "2025-12-06 12:57:38,786 - src.persistence - INFO - Fetched 10 records for user user001\n",
            "2025-12-06 12:57:38,788 - src.persistence - INFO - Fetched 10 records for user user001\n",
            "2025-12-06 12:57:38,791 - src.persistence - INFO - Fetched 10 records for user user002\n",
            "2025-12-06 12:57:38,793 - src.persistence - INFO - Fetched 10 records for user user001\n",
            "2025-12-06 12:57:38,796 - src.persistence - INFO - Fetched 10 records for user user003\n",
            "2025-12-06 12:57:38,798 - src.persistence - INFO - Fetched 10 records for user user002\n",
            "2025-12-06 12:57:38,801 - src.persistence - INFO - Fetched 10 records for user user001\n",
            "2025-12-06 12:57:38,803 - src.persistence - INFO - Fetched 10 records for user user003\n",
            "2025-12-06 12:57:38,806 - src.persistence - INFO - Fetched 10 records for user user002\n",
            "2025-12-06 12:57:38,808 - src.persistence - INFO - Fetched 10 records for user user001\n",
            "2025-12-06 12:57:38,811 - src.persistence - INFO - Fetched 10 records for user user003\n",
            "2025-12-06 12:57:38,813 - src.persistence - INFO - Fetched 10 records for user user002\n",
            "2025-12-06 12:57:38,816 - src.persistence - INFO - Fetched 10 records for user user001\n",
            "2025-12-06 12:57:38,818 - src.persistence - INFO - Fetched 10 records for user user003\n",
            "2025-12-06 12:57:38,820 - src.persistence - INFO - Fetched 10 records for user user002\n",
            "2025-12-06 12:57:38,822 - src.persistence - INFO - Fetched 10 records for user user001\n",
            "2025-12-06 12:57:38,824 - src.persistence - INFO - Fetched 10 records for user user003\n",
            "2025-12-06 12:57:38,827 - src.persistence - INFO - Fetched 10 records for user user002\n",
            "2025-12-06 12:57:38,829 - src.persistence - INFO - Fetched 10 records for user user001\n",
            "2025-12-06 12:57:38,831 - src.persistence - INFO - Fetched 10 records for user user003\n",
            "2025-12-06 12:57:38,832 - src.contextualize - INFO - Sequence creation complete\n",
            "2025-12-06 12:57:38,835 - src.utils - INFO - Checkpoint saved: /Users/vikranthnara/Documents/GitHub/Emotix/checkpoints/contextualization_20251206_125738.parquet (20 rows)\n",
            "2025-12-06 12:57:38,836 - src.pipeline - INFO - [4/5] Modeling...\n",
            "2025-12-06 12:57:38,836 - src.modeling - INFO - Loading model: j-hartmann/emotion-english-distilroberta-base on cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running complete Phase 2 pipeline...\n",
            "This may take a few minutes to download the model on first run.\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc32fd8e87ab4e5fa7c5e11f17d42cd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/294 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "299b15f9150745deb9603f530de60874",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e90c7b404e9a46f1aafb83e7c07d8787",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01a9e61d046c4a5c85e1cd80e755ab23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "282df0600ed3405cb4884498667f97b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfbcafc57db74950bfef0969d5457c7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bdf96edd2ab4cf9927e740c17b6b2ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "/Users/vikranthnara/Documents/GitHub/Emotix/.venv/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n",
            "2025-12-06 12:57:45,616 - src.modeling - INFO - Model loaded successfully\n",
            "2025-12-06 12:57:45,616 - src.modeling - INFO - Running inference on 20 sequences\n",
            "2025-12-06 12:57:45,616 - src.modeling - INFO - Predicting emotions for 20 texts\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06df3dddb810496dbeb5bc4a2cfdc841",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/329M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-06 12:57:46,033 - src.modeling - INFO - Completed predictions for 20 texts\n",
            "2025-12-06 12:57:46,425 - src.modeling - INFO - Inference complete\n",
            "2025-12-06 12:57:46,428 - src.utils - INFO - Checkpoint saved: /Users/vikranthnara/Documents/GitHub/Emotix/checkpoints/modeling_20251206_125746.parquet (20 rows)\n",
            "2025-12-06 12:57:46,429 - src.pipeline - INFO - [5/5] Persistence...\n",
            "2025-12-06 12:57:46,432 - src.persistence - INFO - Committed batch: 20 rows (total: 20)\n",
            "2025-12-06 12:57:46,433 - src.persistence - INFO - Successfully wrote 20 rows to database\n",
            "2025-12-06 12:57:46,433 - src.pipeline - INFO - Pipeline complete. Wrote 20 rows to database.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ“ Pipeline complete in 7.71 seconds\n",
            "âœ“ Processed 20 records\n"
          ]
        }
      ],
      "source": [
        "# Run complete pipeline\n",
        "data_path = project_root / \"data\" / \"sample_data.csv\"\n",
        "db_path = project_root / \"data\" / \"mwb_log.db\"\n",
        "slang_dict_path = project_root / \"data\" / \"slang_dictionary.json\"\n",
        "checkpoint_dir = project_root / \"checkpoints\"\n",
        "\n",
        "print(\"Running complete Phase 2 pipeline...\")\n",
        "print(\"This may take a few minutes to download the model on first run.\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "df_results = run_full_pipeline(\n",
        "    input_path=data_path,\n",
        "    db_path=db_path,\n",
        "    slang_dict_path=slang_dict_path,\n",
        "    model_name=\"j-hartmann/emotion-english-distilroberta-base\",\n",
        "    max_context_turns=5,\n",
        "    batch_size=16,\n",
        "    checkpoint_dir=checkpoint_dir,\n",
        "    archive_raw=True\n",
        ")\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"\\nâœ“ Pipeline complete in {elapsed:.2f} seconds\")\n",
        "print(f\"âœ“ Processed {len(df_results)} records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: View Results\n",
        "\n",
        "Examine the emotion predictions and intensity scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample predictions:\n",
            "\n",
            "    UserID                                             Text  \\\n",
            "0  user001  Ugh, today was so stressful ðŸ˜« tbh I'm exhausted   \n",
            "1  user001                         lol that's hilarious ðŸ˜‚ðŸ˜‚ðŸ˜‚   \n",
            "2  user002           idk what to do anymore... feeling lost   \n",
            "3  user001                 omg I can't believe it worked! ðŸŽ‰   \n",
            "4  user003                  wtf is wrong with me today? smh   \n",
            "5  user002                    thx for being there for me â¤ï¸   \n",
            "6  user001                         brb need to take a break   \n",
            "7  user003                         feeling anxious af rn...   \n",
            "8  user002                imo this is getting better slowly   \n",
            "9  user001                            yolo let's do this! ðŸ’ª   \n",
            "\n",
            "                                      NormalizedText PrimaryEmotionLabel  \\\n",
            "0  Ugh, today was so stressful tired_face to be h...             sadness   \n",
            "1  laughing out loud that's hilarious face_with_t...             sadness   \n",
            "2   i don't know what to do anymore. .. feeling lost             sadness   \n",
            "3  oh my god I can't believe it worked! party_popper            surprise   \n",
            "4  what the f*** is wrong with me today? shaking ...                fear   \n",
            "5            thanks for being there for me red_heart             sadness   \n",
            "6                 be right back need to take a break            surprise   \n",
            "7          feeling anxious away from keyboard rn. ..                fear   \n",
            "8        in my opinion this is getting better slowly             sadness   \n",
            "9    you only live once let's do this! flexed_biceps            surprise   \n",
            "\n",
            "   IntensityScore_Primary  AmbiguityFlag  \n",
            "0                0.971799              0  \n",
            "1                0.551625              1  \n",
            "2                0.991752              0  \n",
            "3                0.843116              0  \n",
            "4                0.975637              0  \n",
            "5                0.991255              0  \n",
            "6                0.913573              0  \n",
            "7                0.993983              0  \n",
            "8                0.990224              0  \n",
            "9                0.955484              0  \n",
            "\n",
            "============================================================\n",
            "Emotion Distribution:\n",
            "============================================================\n",
            "PrimaryEmotionLabel\n",
            "surprise    7\n",
            "sadness     6\n",
            "fear        5\n",
            "joy         2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "============================================================\n",
            "Intensity Statistics:\n",
            "============================================================\n",
            "count    20.000000\n",
            "mean      0.920115\n",
            "std       0.120340\n",
            "min       0.551625\n",
            "25%       0.907421\n",
            "50%       0.968379\n",
            "75%       0.991379\n",
            "max       0.994342\n",
            "Name: IntensityScore_Primary, dtype: float64\n",
            "\n",
            "============================================================\n",
            "Ambiguity Detection:\n",
            "============================================================\n",
            "Ambiguous cases: 1 / 20\n",
            "Percentage: 5.0%\n"
          ]
        }
      ],
      "source": [
        "# Display results\n",
        "print(\"Sample predictions:\\n\")\n",
        "display_cols = ['UserID', 'Text', 'NormalizedText', 'PrimaryEmotionLabel', \n",
        "                'IntensityScore_Primary', 'AmbiguityFlag']\n",
        "print(df_results[display_cols].head(10))\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Emotion Distribution:\")\n",
        "print(\"=\"*60)\n",
        "print(df_results['PrimaryEmotionLabel'].value_counts())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Intensity Statistics:\")\n",
        "print(\"=\"*60)\n",
        "print(df_results['IntensityScore_Primary'].describe())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Ambiguity Detection:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Ambiguous cases: {df_results['AmbiguityFlag'].sum()} / {len(df_results)}\")\n",
        "print(f\"Percentage: {100 * df_results['AmbiguityFlag'].mean():.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Test Contextualization\n",
        "\n",
        "Compare predictions with and without context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-06 12:57:46,474 - src.persistence - INFO - Database schema initialized\n",
            "2025-12-06 12:57:46,475 - src.modeling - INFO - Loading model: j-hartmann/emotion-english-distilroberta-base on cpu\n",
            "Device set to use cpu\n",
            "/Users/vikranthnara/Documents/GitHub/Emotix/.venv/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n",
            "2025-12-06 12:57:47,108 - src.modeling - INFO - Model loaded successfully\n",
            "2025-12-06 12:57:47,111 - src.persistence - INFO - Fetched 10 records for user user001\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing utterance: 'be right back need to take a break'\n",
            "\n",
            "Without context:\n",
            "  Label: sadness\n",
            "  Intensity: 0.626\n",
            "\n",
            "With context:\n",
            "  Label: surprise\n",
            "  Intensity: 0.953\n",
            "\n",
            "Context used: 9 previous turns\n",
            "Sample context: oh my god I can't believe it worked! party_popper\n"
          ]
        }
      ],
      "source": [
        "# Test contextualization impact\n",
        "persistence = MWBPersistence(db_path)\n",
        "model = EmotionModel()\n",
        "\n",
        "# Get a sample user's recent utterance\n",
        "user_id = \"user001\"\n",
        "history = persistence.fetch_history(user_id, limit=10)\n",
        "\n",
        "if len(history) > 0:\n",
        "    # Get most recent utterance\n",
        "    latest = history.iloc[-1]\n",
        "    utterance = latest['NormalizedText']\n",
        "    \n",
        "    print(f\"Testing utterance: '{utterance}'\\n\")\n",
        "    \n",
        "    # Predict without context\n",
        "    pred_no_context = model.predict_emotion(utterance)\n",
        "    print(\"Without context:\")\n",
        "    print(f\"  Label: {pred_no_context['label']}\")\n",
        "    print(f\"  Intensity: {pred_no_context['intensity']:.3f}\\n\")\n",
        "    \n",
        "    # Predict with context\n",
        "    from src.contextualize import create_sequence_for_model\n",
        "    context_history = history.iloc[:-1]  # All but the last\n",
        "    sequence = create_sequence_for_model(utterance, context_history)\n",
        "    \n",
        "    pred_with_context = model.predict_emotion(sequence)\n",
        "    print(\"With context:\")\n",
        "    print(f\"  Label: {pred_with_context['label']}\")\n",
        "    print(f\"  Intensity: {pred_with_context['intensity']:.3f}\\n\")\n",
        "    \n",
        "    print(f\"Context used: {len(context_history)} previous turns\")\n",
        "    if len(context_history) > 0:\n",
        "        print(f\"Sample context: {context_history['NormalizedText'].iloc[-1]}\")\n",
        "else:\n",
        "    print(\"No history found for user\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Synthetic Ambiguity Dataset\n",
        "\n",
        "Generate and validate on synthetic ambiguous cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-06 12:57:47,198 - src.validation - INFO - Generating 20 ambiguous cases\n",
            "2025-12-06 12:57:47,199 - src.validation - INFO - Generated 20 ambiguous cases\n",
            "2025-12-06 12:57:47,201 - src.modeling - INFO - Loading model: j-hartmann/emotion-english-distilroberta-base on cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 20 ambiguous cases\n",
            "\n",
            "Sample ambiguous cases:\n",
            "           Text                 PossibleLabels  \\\n",
            "0      I'm fine      [neutral, sadness, anger]   \n",
            "1      Whatever      [neutral, anger, sadness]   \n",
            "2  That's great          [joy, sarcasm, anger]   \n",
            "3  I don't know  [confusion, sadness, anxiety]   \n",
            "4         Maybe  [neutral, anxiety, confusion]   \n",
            "\n",
            "                                Description  \n",
            "0    Sarcastic 'fine' can indicate distress  \n",
            "1     Dismissive response can hide emotions  \n",
            "2        Sarcastic positive can be negative  \n",
            "3  Uncertainty can indicate multiple states  \n",
            "4            Hedging can mask true feelings  \n",
            "\n",
            "============================================================\n",
            "Testing model on ambiguous cases:\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "/Users/vikranthnara/Documents/GitHub/Emotix/.venv/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n",
            "2025-12-06 12:57:47,863 - src.modeling - INFO - Model loaded successfully\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results:\n",
            "           Text                   PossibleLabels PredictedLabel  IsCorrect\n",
            "0      I'm fine        [neutral, sadness, anger]        neutral       True\n",
            "1      Whatever        [neutral, anger, sadness]        neutral       True\n",
            "2  That's great            [joy, sarcasm, anger]            joy       True\n",
            "3  I don't know    [confusion, sadness, anxiety]       surprise      False\n",
            "4         Maybe    [neutral, anxiety, confusion]        neutral       True\n",
            "5       I guess  [neutral, uncertainty, anxiety]        neutral       True\n",
            "6      I'm okay      [neutral, sadness, anxiety]        neutral       True\n",
            "7  It's nothing        [neutral, sadness, anger]        neutral       True\n",
            "8      I'm good          [joy, neutral, sarcasm]            joy       True\n",
            "9     I'm tired       [neutral, sadness, stress]        sadness       True\n",
            "\n",
            "Accuracy on ambiguous cases: 90.00%\n"
          ]
        }
      ],
      "source": [
        "# Generate synthetic ambiguous cases\n",
        "generator = AmbiguityGenerator()\n",
        "ambiguous_df = generator.generate_ambiguous_cases(num_cases=20, include_context=True)\n",
        "\n",
        "print(f\"Generated {len(ambiguous_df)} ambiguous cases\\n\")\n",
        "print(\"Sample ambiguous cases:\")\n",
        "print(ambiguous_df[['Text', 'PossibleLabels', 'Description']].head(5))\n",
        "\n",
        "# Test model on ambiguous cases\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Testing model on ambiguous cases:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model = EmotionModel()\n",
        "ambiguous_results = []\n",
        "\n",
        "for _, row in ambiguous_df.head(10).iterrows():\n",
        "    text = row['Text']\n",
        "    possible_labels = row['PossibleLabels']\n",
        "    \n",
        "    pred = model.predict_emotion(text)\n",
        "    ambiguous_results.append({\n",
        "        'Text': text,\n",
        "        'PossibleLabels': possible_labels,\n",
        "        'PredictedLabel': pred['label'],\n",
        "        'Intensity': pred['intensity'],\n",
        "        'IsCorrect': pred['label'].lower() in [l.lower() for l in possible_labels]\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(ambiguous_results)\n",
        "print(\"\\nResults:\")\n",
        "print(results_df[['Text', 'PossibleLabels', 'PredictedLabel', 'IsCorrect']])\n",
        "\n",
        "accuracy = results_df['IsCorrect'].mean()\n",
        "print(f\"\\nAccuracy on ambiguous cases: {accuracy:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Post-Processing Analysis\n",
        "\n",
        "Analyze the impact of post-processing rules on predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test post-processing on results\n",
        "from src.postprocess import apply_post_processing\n",
        "\n",
        "if 'PrimaryEmotionLabel' in df_results.columns:\n",
        "    df_postprocessed = apply_post_processing(df_results)\n",
        "    \n",
        "    print(\"Post-Processing Results:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total predictions: {len(df_postprocessed)}\")\n",
        "    print(f\"Overrides applied: {df_postprocessed['WasOverridden'].sum()}\")\n",
        "    print(f\"Flagged for review: {df_postprocessed['NeedsReview'].sum()}\")\n",
        "    \n",
        "    if df_postprocessed['WasOverridden'].sum() > 0:\n",
        "        print(\"\\nOverrides Applied:\")\n",
        "        overrides = df_postprocessed[df_postprocessed['WasOverridden'] == 1]\n",
        "        for idx, row in overrides.head(5).iterrows():\n",
        "            print(f\"\\n  '{row['NormalizedText'][:50]}...'\")\n",
        "            print(f\"    {row['PrimaryEmotionLabel']} ({row['IntensityScore_Primary']:.2f}) â†’ {row['CorrectedLabel']} ({row['CorrectedIntensity']:.2f})\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Emotion Distribution (After Post-Processing):\")\n",
        "    print(\"=\"*60)\n",
        "    if 'CorrectedLabel' in df_postprocessed.columns:\n",
        "        print(df_postprocessed['CorrectedLabel'].value_counts())\n",
        "    else:\n",
        "        print(df_postprocessed['PrimaryEmotionLabel'].value_counts())\n",
        "else:\n",
        "    print(\"No predictions found. Run the pipeline first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "âœ… **Complete Pipeline**: All 5 layers integrated  \n",
        "âœ… **Emotion Classification**: DistilRoBERTa-based model for emotion labels  \n",
        "âœ… **Intensity Prediction**: Softmax probabilities â†’ intensity scores (0.0-1.0)  \n",
        "âœ… **Contextualization**: Multi-turn sequences for ambiguity resolution  \n",
        "âœ… **Ambiguity Detection**: Automatic flagging of ambiguous cases  \n",
        "âœ… **Validation**: Synthetic dataset generator for testing  \n",
        "\n",
        "**Next Steps:**\n",
        "- Fine-tune model on labeled MWB dataset\n",
        "- Expand ambiguity detection with more sophisticated methods\n",
        "- Implement human-in-the-loop feedback mechanism\n",
        "- Add real-time inference API\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
