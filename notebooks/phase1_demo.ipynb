{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1 Demo: MWB Tracking Pipeline\n",
        "\n",
        "This notebook demonstrates the end-to-end Phase 1 pipeline:\n",
        "1. **Ingestion**: Load raw conversational text from CSV\n",
        "2. **Preprocessing**: Normalize slang, emojis, and apply fixes\n",
        "3. **Persistence**: Write to SQLite and demonstrate history retrieval\n",
        "\n",
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-06 02:17:51,417 - root - INFO - Starting Phase 1 Demo\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "from src.ingest import ingest_csv\n",
        "from src.preprocess import preprocess_pipeline\n",
        "from src.persistence import MWBPersistence\n",
        "from src.utils import setup_logging, checkpoint_dataframe\n",
        "\n",
        "# Setup logging\n",
        "setup_logging(log_level=\"INFO\")\n",
        "logging.info(\"Starting Phase 1 Demo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Ingestion\n",
        "\n",
        "Load sample data from CSV file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-06 02:17:51,422 - src.ingest - INFO - Ingesting CSV from /Users/vikranthnara/Documents/GitHub/Emotix/data/sample_data.csv\n",
            "2025-12-06 02:17:51,428 - src.ingest - INFO - Validated DataFrame with 20 rows\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 20 records\n",
            "\n",
            "First few rows:\n",
            "    UserID                                             Text  \\\n",
            "0  user001  Ugh, today was so stressful ðŸ˜« tbh I'm exhausted   \n",
            "1  user001                         lol that's hilarious ðŸ˜‚ðŸ˜‚ðŸ˜‚   \n",
            "2  user002           idk what to do anymore... feeling lost   \n",
            "3  user001                 omg I can't believe it worked! ðŸŽ‰   \n",
            "4  user003                  wtf is wrong with me today? smh   \n",
            "\n",
            "            Timestamp  \n",
            "0 2024-01-15 10:30:00  \n",
            "1 2024-01-15 11:15:00  \n",
            "2 2024-01-15 12:00:00  \n",
            "3 2024-01-15 13:45:00  \n",
            "4 2024-01-15 14:20:00  \n",
            "\n",
            "Columns: ['UserID', 'Text', 'Timestamp']\n",
            "\n",
            "Data types:\n",
            "UserID               object\n",
            "Text                 object\n",
            "Timestamp    datetime64[ns]\n",
            "dtype: object\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-06 02:17:52,167 - src.utils - INFO - Checkpoint saved: /Users/vikranthnara/Documents/GitHub/Emotix/checkpoints/ingestion_20251206_021751.parquet (20 rows)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PosixPath('/Users/vikranthnara/Documents/GitHub/Emotix/checkpoints/ingestion_20251206_021751.parquet')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ingest sample data\n",
        "data_path = project_root / \"data\" / \"sample_data.csv\"\n",
        "df_raw = ingest_csv(data_path)\n",
        "\n",
        "print(f\"Loaded {len(df_raw)} records\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_raw.head())\n",
        "print(f\"\\nColumns: {list(df_raw.columns)}\")\n",
        "print(f\"\\nData types:\\n{df_raw.dtypes}\")\n",
        "\n",
        "# Checkpoint after ingestion\n",
        "checkpoint_dir = project_root / \"checkpoints\"\n",
        "checkpoint_dataframe(df_raw, checkpoint_dir, \"ingestion\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Preprocessing\n",
        "\n",
        "Apply normalization pipeline: slang dictionary lookup, emoji demojization, and syntactic fixes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-06 02:17:52,174 - src.preprocess - INFO - Loaded 33 slang/abbrev mappings\n",
            "2025-12-06 02:17:52,176 - src.preprocess - INFO - Preprocessing 20 rows\n",
            "2025-12-06 02:17:52,184 - src.preprocess - INFO - Preprocessing complete\n",
            "2025-12-06 02:17:52,187 - src.utils - INFO - Checkpoint saved: /Users/vikranthnara/Documents/GitHub/Emotix/checkpoints/preprocessing_20251206_021752.parquet (20 rows)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed 20 records\n",
            "\n",
            "Sample transformations:\n",
            "\n",
            "Original: Ugh, today was so stressful ðŸ˜« tbh I'm exhausted\n",
            "Normalized: Ugh, today was so stressful tired_face to be honest in my opinion exhausted\n",
            "  Slang replaced: [{'original': 'tbh', 'replacement': 'to be honest'}]\n",
            "  Emojis found: ['ðŸ˜«']\n",
            "\n",
            "Original: lol that's hilarious ðŸ˜‚ðŸ˜‚ðŸ˜‚\n",
            "Normalized: laughing out loud that's hilarious face_with_tears_of_joy face_with_tears_of_joy face_with_tears_of_joy\n",
            "  Slang replaced: [{'original': 'lol', 'replacement': 'laughing out loud'}]\n",
            "  Emojis found: ['ðŸ˜‚']\n",
            "\n",
            "Original: idk what to do anymore... feeling lost\n",
            "Normalized: i don't know what to do anymore. .. feeling lost\n",
            "  Slang replaced: [{'original': 'idk', 'replacement': \"i don't know\"}]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PosixPath('/Users/vikranthnara/Documents/GitHub/Emotix/checkpoints/preprocessing_20251206_021752.parquet')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load slang dictionary and preprocess\n",
        "slang_dict_path = project_root / \"data\" / \"slang_dictionary.json\"\n",
        "df_preprocessed = preprocess_pipeline(df_raw, slang_dict_path=slang_dict_path)\n",
        "\n",
        "print(f\"Preprocessed {len(df_preprocessed)} records\")\n",
        "print(\"\\nSample transformations:\")\n",
        "sample = df_preprocessed[['Text', 'NormalizedText', 'NormalizationFlags']].head(3)\n",
        "for idx, row in sample.iterrows():\n",
        "    print(f\"\\nOriginal: {row['Text']}\")\n",
        "    print(f\"Normalized: {row['NormalizedText']}\")\n",
        "    flags = row['NormalizationFlags']\n",
        "    if flags.get('slang_replacements'):\n",
        "        print(f\"  Slang replaced: {flags['slang_replacements']}\")\n",
        "    if flags.get('emojis_found'):\n",
        "        print(f\"  Emojis found: {flags['emojis_found']}\")\n",
        "\n",
        "# Checkpoint after preprocessing\n",
        "checkpoint_dataframe(df_preprocessed, checkpoint_dir, \"preprocessing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Persistence\n",
        "\n",
        "Write preprocessed data to SQLite database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-06 02:17:52,246 - src.persistence - INFO - Database schema initialized\n",
            "2025-12-06 02:17:52,251 - src.persistence - INFO - Committed batch: 20 rows (total: 20)\n",
            "2025-12-06 02:17:52,251 - src.persistence - INFO - Successfully wrote 20 rows to database\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully wrote 20 rows to database\n",
            "Database location: /Users/vikranthnara/Documents/GitHub/Emotix/data/mwb_log.db\n"
          ]
        }
      ],
      "source": [
        "# Initialize persistence layer\n",
        "db_path = project_root / \"data\" / \"mwb_log.db\"\n",
        "persistence = MWBPersistence(db_path)\n",
        "\n",
        "# Write results (with raw text archiving)\n",
        "rows_written = persistence.write_results(df_preprocessed, archive_raw=True)\n",
        "print(f\"Successfully wrote {rows_written} rows to database\")\n",
        "print(f\"Database location: {db_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: History Retrieval\n",
        "\n",
        "Demonstrate fast context retrieval for a user (<100ms requirement).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-06 02:17:52,269 - src.persistence - INFO - Fetched 16 records for user user001\n",
            "2025-12-06 02:17:52,273 - src.persistence - INFO - Fetched 12 records for user user001\n",
            "2025-12-06 02:17:52,275 - src.persistence - INFO - Fetched 3 records for user user001\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved 16 records for user user001 in 2.85ms\n",
            "\n",
            "History (ordered by timestamp):\n",
            "             Timestamp                                     NormalizedText  \\\n",
            "0  2024-01-15 10:30:00  Ugh, today was so stressful tired_face to be h...   \n",
            "1  2024-01-15 10:30:00  Ugh, today was so stressful tired_face to be h...   \n",
            "2  2024-01-15 11:15:00  laughing out loud that's hilarious face_with_t...   \n",
            "3  2024-01-15 11:15:00  laughing out loud that's hilarious face_with_t...   \n",
            "4  2024-01-15 13:45:00  oh my god I can't believe it worked! party_popper   \n",
            "5  2024-01-15 13:45:00  oh my god I can't believe it worked! party_popper   \n",
            "6  2024-01-15 16:00:00                 be right back need to take a break   \n",
            "7  2024-01-15 16:00:00                 be right back need to take a break   \n",
            "8  2024-01-15 19:15:00    you only live once let's do this! flexed_biceps   \n",
            "9  2024-01-15 19:15:00    you only live once let's do this! flexed_biceps   \n",
            "\n",
            "                                  NormalizationFlags  \n",
            "0  {'emojis_found': ['ðŸ˜«'], 'slang_replacements': ...  \n",
            "1  {'emojis_found': ['ðŸ˜«'], 'slang_replacements': ...  \n",
            "2  {'emojis_found': ['ðŸ˜‚'], 'slang_replacements': ...  \n",
            "3  {'emojis_found': ['ðŸ˜‚'], 'slang_replacements': ...  \n",
            "4  {'emojis_found': ['ðŸŽ‰'], 'slang_replacements': ...  \n",
            "5  {'emojis_found': ['ðŸŽ‰'], 'slang_replacements': ...  \n",
            "6  {'emojis_found': [], 'slang_replacements': [{'...  \n",
            "7  {'emojis_found': [], 'slang_replacements': [{'...  \n",
            "8  {'emojis_found': ['ðŸ’ª'], 'slang_replacements': ...  \n",
            "9  {'emojis_found': ['ðŸ’ª'], 'slang_replacements': ...  \n",
            "\n",
            "Records since 2024-01-15 12:00:00: 12\n",
            "\n",
            "Last 3 records:\n",
            "             Timestamp                                     NormalizedText\n",
            "0  2024-01-15 10:30:00  Ugh, today was so stressful tired_face to be h...\n",
            "1  2024-01-15 10:30:00  Ugh, today was so stressful tired_face to be h...\n",
            "2  2024-01-15 11:15:00  laughing out loud that's hilarious face_with_t...\n"
          ]
        }
      ],
      "source": [
        "# Fetch history for user001\n",
        "user_id = \"user001\"\n",
        "\n",
        "start_time = time.time()\n",
        "history = persistence.fetch_history(user_id)\n",
        "elapsed_ms = (time.time() - start_time) * 1000\n",
        "\n",
        "print(f\"Retrieved {len(history)} records for user {user_id} in {elapsed_ms:.2f}ms\")\n",
        "print(f\"\\nHistory (ordered by timestamp):\")\n",
        "print(history[['Timestamp', 'NormalizedText', 'NormalizationFlags']].head(10))\n",
        "\n",
        "# Test with timestamp filter\n",
        "since = datetime(2024, 1, 15, 12, 0, 0)\n",
        "history_filtered = persistence.fetch_history(user_id, since_timestamp=since)\n",
        "print(f\"\\nRecords since {since}: {len(history_filtered)}\")\n",
        "\n",
        "# Test with limit\n",
        "history_limited = persistence.fetch_history(user_id, limit=3)\n",
        "print(f\"\\nLast 3 records:\")\n",
        "print(history_limited[['Timestamp', 'NormalizedText']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Verify Database Schema\n",
        "\n",
        "Check that all required columns are present in the database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MWB Log Schema:\n",
            "  LogID (INTEGER)\n",
            "  UserID (TEXT)\n",
            "  Timestamp (DATETIME)\n",
            "  NormalizedText (TEXT)\n",
            "  PrimaryEmotionLabel (TEXT)\n",
            "  IntensityScore_Primary (REAL)\n",
            "  AmbiguityFlag (INTEGER)\n",
            "  NormalizationFlags (TEXT)\n",
            "  CreatedAt (DATETIME)\n",
            "\n",
            "Total records in mwb_log: 40\n",
            "Total records in raw_archive: 40\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(str(db_path))\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Check schema\n",
        "cursor.execute(\"PRAGMA table_info(mwb_log)\")\n",
        "columns = cursor.fetchall()\n",
        "print(\"MWB Log Schema:\")\n",
        "for col in columns:\n",
        "    print(f\"  {col[1]} ({col[2]})\")\n",
        "\n",
        "# Sample query\n",
        "cursor.execute(\"SELECT COUNT(*) FROM mwb_log\")\n",
        "total = cursor.fetchone()[0]\n",
        "print(f\"\\nTotal records in mwb_log: {total}\")\n",
        "\n",
        "cursor.execute(\"SELECT COUNT(*) FROM raw_archive\")\n",
        "total_archive = cursor.fetchone()[0]\n",
        "print(f\"Total records in raw_archive: {total_archive}\")\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "âœ… **Ingestion**: Successfully loaded and validated 20 records  \n",
        "âœ… **Preprocessing**: Applied normalization (slang, emojis, fixes) with flags tracking  \n",
        "âœ… **Persistence**: Wrote structured data to SQLite with ACID transactions  \n",
        "âœ… **History Retrieval**: Fast context retrieval (<100ms) with indexing  \n",
        "\n",
        "**Next Steps (Phase 2):**\n",
        "- Layer 3: Contextualization (multi-turn sequence creation)\n",
        "- Layer 4: Modeling (fine-tuned DistilRoBERTa inference)\n",
        "- Synthetic ambiguity dataset generation\n",
        "- Validation metrics tracking\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
